{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot4 as uproot\n",
    "import numpy as np\n",
    "import awkward1 as ak\n",
    "import coffea as cf\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import pandas as pd\n",
    "from   sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1=\"/afs/cern.ch/user/x/xcoubez/public/For/ForDilan/JetTree_mc_TTHad.root\"\n",
    "filename=\"/afs/cern.ch/user/x/xcoubez/public/For/ForDilan/JetTree_mc.root\"\n",
    "f1= uproot.open(filename)\n",
    "events1 = uproot.open(filename)[\"btagana/ttree;1\"]\n",
    "\n",
    "f= uproot.open(filename)\n",
    "events = uproot.open(filename)[\"btagana/ttree;1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ak_into_np(ak_array):\n",
    "    data=np.dstack([ak.to_numpy(x) for x in ak_array])\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def extend(track,jet,padding_size):\n",
    "    event,jets,var=np.shape(jet)\n",
    "    jet_to_track_data=[]\n",
    "    for i in jet:\n",
    "        event_jet_to_track=[]\n",
    "        lasttrack=0\n",
    "        for k in i:\n",
    "            if k[4]!=0:\n",
    "                \n",
    "                x=np.array([k[l] for l in range(var-2)])\n",
    "                shape=(int(k[var-1]-k[var-2]),var-2)\n",
    "                \n",
    "                value=np.broadcast_to(x,shape)\n",
    "                event_jet_to_track.append(value)\n",
    "                lasttrack=k[var-1]\n",
    "            else:\n",
    "                x=np.array((var-2)*[0])\n",
    "                shape=(int(padding_size-lasttrack),var-2)\n",
    "                \n",
    "                value=np.broadcast_to(x,shape)\n",
    "                \n",
    "                event_jet_to_track.append(value)\n",
    "                break\n",
    "        \n",
    "        jet_to_track_data.append(event_jet_to_track)\n",
    "    jet_to_track_data=[np.vstack(x) for x in jet_to_track_data]\n",
    "    jet_to_track_data=np.stack(jet_to_track_data)\n",
    "    extended=np.concatenate([track,jet_to_track_data],axis=2)\n",
    "    return extended\n",
    "def get_matrix(svdata):\n",
    "    svdata=np.squeeze(svdata)\n",
    "    matrix = svdata[:,:,None]==svdata[:,None,:]\n",
    "    return matrix.astype(int)\n",
    "    \n",
    "\n",
    "def get_data(filename,padding_size):\n",
    "    f=uproot.open(filename)\n",
    "    events= f[\"btagana/ttree;1\"]\n",
    "    track_data=events.arrays(filter_name=[\"Track_pt\",\"Track_phi\",\"Track_eta\",\"Track_dxy\",\"Track_dz\",\"Track_charge\"])\n",
    "    jet_data=events.arrays(filter_name=[\"Jet_nFirstTrack\",\"Jet_nLastTrack\",\"Jet_pt\",\"Jet_phi\",\"Jet_eta\",\"Jet_looseID\"])\n",
    "    output_data=events.arrays(filter_name=[\"Track_SV\"])\n",
    "    arrays_track=ak.unzip(ak.fill_none(ak.pad_none(track_data, padding_size), 0))\n",
    "    arrays_jet=ak.unzip(ak.fill_none(ak.pad_none(jet_data,padding_size),0))\n",
    "    arrays_sv=ak.unzip(ak.fill_none(ak.pad_none(output_data,padding_size),0))\n",
    "    track=ak_into_np(arrays_track)\n",
    "    jet=ak_into_np(arrays_jet)\n",
    "    output=ak_into_np(arrays_sv)\n",
    "    #output=get_matrix(output)\n",
    "    track_extended=extend(track,jet,padding_size)\n",
    "    return track_extended,output\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal(inp):\n",
    "    Batches=[]\n",
    "    for batch in inp:\n",
    "        diagonalpart=tf.linalg.diag_part(batch)\n",
    "        Batches.append(diagonalpart)\n",
    "    output=tf.stack(Batches)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_diagonal(inp):\n",
    "    Batches=[]\n",
    "    for batch in inp:\n",
    "        diag_matrix=tf.linalg.diag(batch)\n",
    "        Batches.append(diag_matrix)\n",
    "    output=tf.stack(Batches)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 403 ms, sys: 224 ms, total: 627 ms\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputdata,outputdata=get_data(filename1,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finished(with diagonal values)\n",
    "\n",
    "def get_loss(y_hat, y):\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(y_hat,y)  # cross entropy (but no logits)\n",
    "\n",
    "    \n",
    "    y_hat = tf.math.sigmoid(y_hat)\n",
    "    \n",
    "    tp = tf.math.reduce_sum(tf.multiply(y_hat, y),[1,2])\n",
    "    fn = tf.math.reduce_sum((y - tf.multiply(y_hat, y)),[1,2])\n",
    "    fp = tf.math.reduce_sum((y_hat -tf.multiply(y_hat,y)),[1,2])\n",
    "    loss = loss - ((2 * tp) / tf.math.reduce_sum((2 * tp + fp + fn + 1e-10)))  # fscore\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finished\n",
    "\n",
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Attention, self).__init__()\n",
    "        in_features= input_shape[-1]\n",
    "        small_in_features = max(math.floor(in_features/10), 1)\n",
    "        self.d_k = small_in_features\n",
    "\n",
    "        query = tf.keras.models.Sequential()\n",
    "        query.add(tf.keras.layers.Input(input_shape))\n",
    "        query.add(tf.keras.layers.Dense(in_features,use_bias=True,trainable=True))\n",
    "        query.add(tf.keras.layers.Dense(small_in_features,activation=\"tanh\",trainable=True))\n",
    "        self.query= query\n",
    "        \n",
    "        self.key = tf.keras.layers.Dense(small_in_features,use_bias=True,trainable=True)\n",
    "\n",
    "    def call(self, inp):\n",
    "        # inp.shape should be (B,N,C)\n",
    "        q = self.query(inp)  # (B,N,C/10)\n",
    "        k = self.key(inp)     # B,N,C/10\n",
    "        k = tf.transpose(k,perm=[0,2,1])\n",
    "        x = tf.linalg.matmul(q, k) / math.sqrt(self.d_k)  # B,N,N\n",
    "        x = tf.nn.softmax(x)  # over rows\n",
    "        x = tf.linalg.matmul(x, inp)  # (B, N, C)\n",
    "        print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finished\n",
    "\n",
    "class DiagOffdiagMLP(tf.keras.Model):\n",
    "    def __init__(self, input_shape, out_features, seperate_diag):\n",
    "        super(DiagOffdiagMLP, self).__init__()\n",
    "        self.seperate_diag = seperate_diag\n",
    "        self.conv_offdiag = tf.keras.layers.Conv2D(out_features,kernel_size=1)\n",
    "        if self.seperate_diag:\n",
    "            self.conv_diag = tf.keras.layers.Conv1D(out_features, kernel_size=1)\n",
    "    def call(self, inp):\n",
    "        # Assume x.shape == (B, C, N, N)\n",
    "        inp_diag=diagonal(inp)\n",
    "        if self.seperate_diag:\n",
    "            return self.conv_offdiag(inp) + embed_diagonal(self.conv_diag(inp_diag))\n",
    "        return self.conv_offdiag(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finished\n",
    "\n",
    "class DeepSetLayer(tf.keras.Model):\n",
    "    def __init__(self, input_shape, out_features, attention, normalization, second_bias):\n",
    "        \"\"\"\n",
    "        DeepSets single layer\n",
    "        :param input:shape: input's shape\n",
    "        :param out_features: output's number of features\n",
    "        :param attention: Whether to use attention\n",
    "        :param normalization: normalization method - 'fro' or 'batchnorm'\n",
    "        :param second_bias: use a bias in second conv1d layer\n",
    "        \"\"\"\n",
    "        super(DeepSetLayer, self).__init__()\n",
    "        in_features=input_shape[-1]\n",
    "        self.attention = None\n",
    "        if attention:\n",
    "            self.Attention = Attention(input_shape)\n",
    "        print(out_features)\n",
    "        self.layer1 = keras.models.Sequential(tf.keras.layers.Conv1D(out_features, kernel_size=1))\n",
    "        self.layer2 = keras.models.Sequential(tf.keras.layers.Conv1D(out_features, kernel_size=1, use_bias=second_bias))\n",
    "\n",
    "        self.normalization = normalization\n",
    "        if normalization == 'batchnorm':\n",
    "            self.bn = keras.models.Sequential(tf.keras.layers.BatchNormalization(out_features))\n",
    "\n",
    "    def call(self, x):\n",
    "        #tf.shape(x) = (B,C,N)\n",
    "        # attention\n",
    "        if self.attention:\n",
    "            x_T = tf.transpose(x,perm=[0,2,1])  # B,C,N -> B,N,C\n",
    "            x = self.layer1(x) + self.layer2(tf.transpose(self.Attention(x_T),perm=[0,1,2]))\n",
    "            \n",
    "        else:\n",
    "            x = self.layer1(x) + self.layer2(x - tf.math.reduce_mean(x,axis=2,keepdims=True))\n",
    "        \n",
    "\n",
    "        # normalization\n",
    "        if self.normalization == 'batchnorm':\n",
    "            x = self.bn(x)\n",
    "        else:\n",
    "            x=tf.transpose(x,perm=[0,1,2])\n",
    "            x = x / tf.norm(x,ord=\"fro\" ,axis=[1,2], keepdims=True)  # BxCxN / Bx1xN\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finished\n",
    "\n",
    "class PsiSuffix(tf.keras.Model):\n",
    "    def __init__(self, features, predict_diagonal):\n",
    "        super(PsiSuffix,self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(features) - 2):\n",
    "            layers.append(DiagOffdiagMLP(features[i], features[i + 1], predict_diagonal))\n",
    "            layers.append(tf.keras.layers.ReLU())\n",
    "        layers.append(DiagOffdiagMLP((None,features[-2]), features[-1], predict_diagonal))\n",
    "        self.model = tf.keras.models.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(tf.keras.Model):\n",
    "    def __init__(self, input_shape, feats, attention, cfg=None):\n",
    "        \"\"\"\n",
    "        DeepSets implementation\n",
    "        :param in_features: input's number of features\n",
    "        :param feats: list of features for each deepsets layer\n",
    "        :param attention: True/False to use attention\n",
    "        :param cfg: configurations of second_bias and normalization method\n",
    "        \"\"\"\n",
    "        super(DeepSet, self).__init__()\n",
    "        if cfg is None:\n",
    "            cfg = {}\n",
    "\n",
    "        layers = [tf.keras.layers.InputLayer(input_shape)]\n",
    "        normalization = cfg.get('normalization', 'fro')\n",
    "        second_bias = cfg.get('second_bias', True)\n",
    "\n",
    "        layers.append(DeepSetLayer(input_shape, feats[0], attention, normalization, second_bias))\n",
    "        for i in range(1, len(feats)):\n",
    "            layers.append(tf.keras.layers.ReLU())\n",
    "            layers.append(DeepSetLayer((None,feats[i-1]), feats[i], attention, normalization, second_bias))\n",
    "\n",
    "        self.sequential = tf.keras.models.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetToGraph(tf.keras.Model):\n",
    "    def __init__(self, input_shape, out_features, set_fn_feats, method, hidden_mlp, predict_diagonal, attention, cfg=None):\n",
    "        \"\"\"\n",
    "        SetToGraph model.\n",
    "        :param in_features: input set's number of features per data point\n",
    "        :param out_features: number of output features.\n",
    "        :param set_fn_feats: list of number of features for the output of each deepsets layer\n",
    "        :param method: transformer method - quad, lin2 or lin5\n",
    "        :param hidden_mlp: list[int], number of features in hidden layers mlp.\n",
    "        :param predict_diagonal: Bool. True to predict the diagonal (diagonal needs a separate psi function).\n",
    "        :param attention: Bool. Use attention in DeepSets\n",
    "        :param cfg: configurations of using second bias in DeepSetLayer, normalization method and aggregation for lin5.\n",
    "        \"\"\"\n",
    "        super(SetToGraph, self).__init__()\n",
    "        assert method in ['lin2', 'lin5']\n",
    "        in_features=input_shape[-1]\n",
    "        self.method = method\n",
    "        if cfg is None:\n",
    "            cfg = {}\n",
    "        self.agg = cfg.get('agg', tf.math.reduce_sum)\n",
    "\n",
    "        self.set_model = DeepSet(input_shape, feats=set_fn_feats, attention=attention, cfg=cfg)\n",
    "\n",
    "        # Suffix - from last number of features, to 1 feature per entrance\n",
    "        d2 = (2 if method == 'lin2' else 5) * set_fn_feats[-1]\n",
    "        hidden_mlp = [d2] + hidden_mlp + [out_features]\n",
    "        self.suffix = PsiSuffix(hidden_mlp, predict_diagonal=predict_diagonal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2, 1)  # from BxNxC to BxCxN\n",
    "        x=tf.transpose(x,perm=[0,2,1])\n",
    "        u = self.set_model(x)  # Bx(out_features)xN\n",
    "        n = tf.shape(u)[2]\n",
    "\n",
    "        if self.method == 'lin2':\n",
    "            m1 = tf.tile(tf.expand_dims(u,2),[1, 1, n, 1])  # broadcast to rows\n",
    "            m2 = tf.tile(tf.expand_dims(u,3),[1, 1, 1, n]) # broadcast to cols\n",
    "            block = torch.cat((m1, m2), dim=1)\n",
    "        elif self.method == 'lin5':\n",
    "            m1 = tf.tile(tf.expand_dims(u,2),[1, 1, n, 1])  # broadcast to rows\n",
    "            m2 = tf.tile(tf.expand_dims(u,3),[1, 1, 1, n])  # broadcast to cols\n",
    "            m3 = tf.tile(tf.expand_dims(self.agg(u, axis=2, keepdims=True),3),[1,1,n,n])  # sum over N, put on all\n",
    "            m4 = embed_diagonal(u)  # assign values to diag only\n",
    "            m5 = embed_diagonal(tf.tile(tf.expand_dims(self.agg(u, axis=2, keepdims=True),3),[1,1,n]))  # sum over N, put on diag\n",
    "            block = tf.concat([m1, m2, m3, m4, m5], axis=1)\n",
    "        edge_vals = self.suffix(block)  # shape (B,out_features,N,N)\n",
    "\n",
    "        return edge_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "phi=SetToGraph((256,10),out_features=1,\n",
    "                set_fn_feats=[256,256, 256,256, 5],\n",
    "                method=\"lin2\",\n",
    "                hidden_mlp=[256],\n",
    "                predict_diagonal=False,\n",
    "                attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi.compile(optimizer=\"adam\",metrics=[\"accuracy\"],loss=get_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:714 call  **\n        raise NotImplementedError('When subclassing the `Model` class, you should'\n\n    NotImplementedError: When subclassing the `Model` class, you should implement a `call` method.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-923e51573850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /afs/cern.ch/user/d/dkarim/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:714 call  **\n        raise NotImplementedError('When subclassing the `Model` class, you should'\n\n    NotImplementedError: When subclassing the `Model` class, you should implement a `call` method.\n"
     ]
    }
   ],
   "source": [
    "phi.fit(inputdata, outputdata, epochs=10 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
